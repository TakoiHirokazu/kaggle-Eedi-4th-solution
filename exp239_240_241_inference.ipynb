{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83eaa1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# libraries\n",
    "# =========================\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "import sys\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import random\n",
    "import os\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from transformers import set_seed, AutoConfig, AutoModel, MistralPreTrainedModel, MistralConfig, DynamicCache, \\\n",
    "    Cache\n",
    "from typing import List, Tuple, Optional, Union\n",
    "from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask_for_sdpa, \\\n",
    "    _prepare_4d_causal_attention_mask\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "from transformers import BitsAndBytesConfig\n",
    "# from transformers.models.mistral.modeling_flax_mistral import MISTRAL_INPUTS_DOCSTRING\n",
    "from transformers.models.mistral.modeling_mistral import MistralDecoderLayer, MistralRMSNorm\n",
    "from transformers.utils import add_start_docstrings_to_model_forward\n",
    "from torch import nn, Tensor\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType,PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7372de45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 10:04:45,306 - INFO - logger set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# constants\n",
    "# =========================\n",
    "DATA_DIR = Path(\"/tmp/working/data\")\n",
    "OUTPUT_DIR = Path(\"/tmp/working/storage/eedi/output/\")\n",
    "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
    "MISCONCEPTION_MAPPING_PATH = DATA_DIR / \"misconception_mapping.csv\"\n",
    "LLM_TEXT_PATH = Path(\n",
    "    \"/tmp/working/output/kaggle/exp105/exp105_train_add_text.csv\")\n",
    "FOLD_PATH = \"/tmp/working/output/team/eedi_fold.csv\"\n",
    "\n",
    "# =========================\n",
    "# settings\n",
    "# =========================\n",
    "exp = \"239\"\n",
    "exp_dir = OUTPUT_DIR / \"exp\" / f\"ex{exp}\"\n",
    "model_dir = exp_dir / \"model\"\n",
    "\n",
    "exp1 = \"240\"\n",
    "exp2 = \"241\"\n",
    "\n",
    "exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "logger_path = exp_dir / f\"ex{exp}.txt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# mdoel settings\n",
    "# =========================\n",
    "\n",
    "seed = 1\n",
    "model_path = \"Salesforce/SFR-Embedding-2_R\"\n",
    "batch_size = 30\n",
    "n_epochs = 10\n",
    "max_len = 144\n",
    "weight_decay = 0.1\n",
    "lr = 1e-4\n",
    "num_warmup_steps_rate = 0.1\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "n_candidate = 25\n",
    "iters_to_accumulate = 1\n",
    "\n",
    "\n",
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class EediDataset(Dataset):\n",
    "    def __init__(self, text1, text2,\n",
    "                 tokenizer, max_len,\n",
    "                 labels=None):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text1)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text1 = self.text1[item]\n",
    "        text2 = self.text2[item]\n",
    "        inputs1 = self.tokenizer(\n",
    "            text1,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        inputs2 = self.tokenizer(\n",
    "            text2,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        inputs1 = {\"input_ids\": torch.tensor(inputs1[\"input_ids\"],\n",
    "                                             dtype=torch.long),\n",
    "                   \"attention_mask\": torch.tensor(inputs1[\"attention_mask\"],\n",
    "                                                  dtype=torch.long),\n",
    "                   \"token_type_ids\": torch.tensor(inputs1[\"token_type_ids\"],\n",
    "                                                  dtype=torch.long)}\n",
    "        inputs2 = {\"input_ids\": torch.tensor(inputs2[\"input_ids\"], dtype=torch.long),\n",
    "                   \"attention_mask\": torch.tensor(inputs2[\"attention_mask\"],\n",
    "                                                  dtype=torch.long),\n",
    "                   \"token_type_ids\": torch.tensor(inputs2[\"token_type_ids\"],\n",
    "                                                  dtype=torch.long)}\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[item]\n",
    "            return {\n",
    "                \"input1\": inputs1,\n",
    "                \"input2\": inputs2,\n",
    "                \"label\": torch.tensor(label, dtype=torch.float32),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input1\": inputs1,\n",
    "                \"input2\": inputs2,\n",
    "            }\n",
    "\n",
    "\n",
    "class EediValDataset(Dataset):\n",
    "    def __init__(self, text1,\n",
    "                 tokenizer, max_len):\n",
    "        self.text1 = text1\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text1)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text1 = self.text1[item]\n",
    "        inputs1 = self.tokenizer(\n",
    "            text1,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        inputs1 = {\"input_ids\": torch.tensor(inputs1[\"input_ids\"], dtype=torch.long),\n",
    "                   \"attention_mask\": torch.tensor(inputs1[\"attention_mask\"],\n",
    "                                                  dtype=torch.long),\n",
    "                   \"token_type_ids\": torch.tensor(inputs1[\"token_type_ids\"],\n",
    "                                                  dtype=torch.long)}\n",
    "\n",
    "        return inputs1\n",
    "\n",
    "\n",
    "class MistralModel(MistralPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`MistralDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: MistralConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: MistralConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [MistralDecoderLayer(config, layer_idx)\n",
    "             for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "        self.norm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        labels: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            batch_size, seq_length = input_ids.shape\n",
    "        elif inputs_embeds is not None:\n",
    "            batch_size, seq_length, _ = inputs_embeds.shape\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                # logger.warning_once(\n",
    "                #     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                # )\n",
    "                use_cache = False\n",
    "\n",
    "        past_key_values_length = 0\n",
    "\n",
    "        if use_cache:\n",
    "            use_legacy_cache = not isinstance(past_key_values, Cache)\n",
    "            if use_legacy_cache:\n",
    "                past_key_values = DynamicCache.from_legacy_cache(\n",
    "                    past_key_values)\n",
    "            past_key_values_length = past_key_values.get_usable_length(\n",
    "                seq_length)\n",
    "\n",
    "        if position_ids is None:\n",
    "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "            position_ids = torch.arange(\n",
    "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
    "            )\n",
    "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "        else:\n",
    "            position_ids = position_ids.view(-1, seq_length).long()\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        if attention_mask is not None and self._attn_implementation == \"flash_attention_2\" and use_cache:\n",
    "            is_padding_right = attention_mask[:, -1].sum().item() != batch_size\n",
    "            if is_padding_right:\n",
    "                raise ValueError(\n",
    "                    \"You are attempting to perform batched generation with padding_side='right'\"\n",
    "                    \" this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to \"\n",
    "                    \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n",
    "                )\n",
    "\n",
    "        if self._attn_implementation == \"flash_attention_2\":\n",
    "            # 2d mask is passed through the layers\n",
    "            attention_mask = attention_mask if (\n",
    "                attention_mask is not None and 0 in attention_mask) else None\n",
    "        elif self._attn_implementation == \"sdpa\" and not output_attentions:\n",
    "            # output_attentions=True can not be supported when using SDPA, and we fall back on\n",
    "            # the manual implementation that requires a 4D causal mask in all cases.\n",
    "            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n",
    "                attention_mask,\n",
    "                (batch_size, seq_length),\n",
    "                inputs_embeds,\n",
    "                past_key_values_length,\n",
    "                sliding_window=self.config.sliding_window,\n",
    "            )\n",
    "        else:\n",
    "            # 4d mask is passed through the layers\n",
    "            attention_mask = _prepare_4d_causal_attention_mask(\n",
    "                attention_mask,\n",
    "                (batch_size, seq_length),\n",
    "                inputs_embeds,\n",
    "                past_key_values_length,\n",
    "                sliding_window=self.config.sliding_window,\n",
    "            )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = None\n",
    "        if use_cache:\n",
    "            next_cache = next_decoder_cache.to_legacy_cache(\n",
    "            ) if use_legacy_cache else next_decoder_cache\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "\n",
    "class BiEncoderModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 sentence_pooling_method: str = \"last\"\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"fp4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "        model = MistralModel.from_pretrained(\n",
    "            model_path, quantization_config=bnb_config)\n",
    "        # model = IgnoreLabelsWrapper(model)\n",
    "        config = LoraConfig(\n",
    "            r=64,\n",
    "            lora_alpha=128,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            bias=\"none\",\n",
    "            lora_dropout=0.05,  # Conventional\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        self.model = get_peft_model(model, config)\n",
    "        # self.model.gradient_checkpointing_enable()\n",
    "        self.model.print_trainable_parameters()\n",
    "        self.sentence_pooling_method = sentence_pooling_method\n",
    "        self.config = self.model.config\n",
    "\n",
    "    def gradient_checkpointing_enable(self, **kwargs):\n",
    "        self.model.gradient_checkpointing_enable(**kwargs)\n",
    "\n",
    "    def last_token_pool(self, last_hidden_states: Tensor,\n",
    "                        attention_mask: Tensor) -> Tensor:\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "            return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "    def sentence_embedding(self, hidden_state, mask):\n",
    "        if self.sentence_pooling_method == 'mean':\n",
    "            s = torch.sum(hidden_state * mask.unsqueeze(-1).float(), dim=1)\n",
    "            d = mask.sum(axis=1, keepdim=True).float()\n",
    "            return s / d\n",
    "        elif self.sentence_pooling_method == 'cls':\n",
    "            return hidden_state[:, 0]\n",
    "        elif self.sentence_pooling_method == 'last':\n",
    "            return self.last_token_pool(hidden_state, mask)\n",
    "\n",
    "    def encode(self, input_is, attention_mask):\n",
    "        # print(features)\n",
    "        psg_out = self.model(input_ids=input_is, attention_mask=attention_mask,\n",
    "                             return_dict=True)\n",
    "        p_reps = self.sentence_embedding(\n",
    "            psg_out.last_hidden_state, attention_mask)\n",
    "        return p_reps.contiguous()\n",
    "\n",
    "    def forward(self, input_is, attention_mask):\n",
    "        q_reps = self.encode(input_is, attention_mask)\n",
    "        return q_reps\n",
    "\n",
    "    def _dist_gather_tensor(self, t: Optional[torch.Tensor]):\n",
    "        if t is None:\n",
    "            return None\n",
    "        t = t.contiguous()\n",
    "\n",
    "        all_tensors = [torch.empty_like(t) for _ in range(self.world_size)]\n",
    "        dist.all_gather(all_tensors, t)\n",
    "\n",
    "        all_tensors[self.process_rank] = t\n",
    "        all_tensors = torch.cat(all_tensors, dim=0)\n",
    "\n",
    "        return all_tensors\n",
    "\n",
    "\n",
    "def get_optimizer_grouped_parameters(\n",
    "        model,\n",
    "        weight_decay,\n",
    "        lora_lr=5e-4,\n",
    "        no_decay_name_list=[\"bias\", \"LayerNorm.weight\"],\n",
    "        lora_name_list=[\"lora_right_weight\", \"lora_left_weight\"],\n",
    "):\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if (not any(nd in n for nd in no_decay_name_list)\n",
    "                    and p.requires_grad and not any(nd in n\n",
    "                                                    for nd in lora_name_list))\n",
    "            ],\n",
    "            \"weight_decay\":\n",
    "                weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if (not any(nd in n for nd in no_decay_name_list)\n",
    "                    and p.requires_grad and any(nd in n\n",
    "                                                for nd in lora_name_list))\n",
    "            ],\n",
    "            \"weight_decay\":\n",
    "                weight_decay,\n",
    "            \"lr\":\n",
    "                lora_lr\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if (any(nd in n\n",
    "                        for nd in no_decay_name_list) and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\":\n",
    "                0.0,\n",
    "        },\n",
    "    ]\n",
    "    if not optimizer_grouped_parameters[1][\"params\"]:\n",
    "        optimizer_grouped_parameters.pop(1)\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "\n",
    "def save_model(output_dir, model, tokenizer, fold):\n",
    "    save_dir = output_dir / f\"fold{fold}\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    # model = convert_lora_to_linear_layer(model)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save = model_to_save.model\n",
    "    # model_to_save.save_pretrained(output_dir)\n",
    "\n",
    "    CONFIG_NAME = \"config.json\"\n",
    "    WEIGHTS_NAME = \"adapter.bin\"\n",
    "    output_model_file = save_dir / WEIGHTS_NAME\n",
    "    save_dict = model_to_save.state_dict()\n",
    "    final_d = {}\n",
    "    for k, v in save_dict.items():\n",
    "        if \"lora\" in k:\n",
    "            final_d[k] = v\n",
    "    torch.save(final_d, output_model_file)\n",
    "    print('saving success')\n",
    "\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "\n",
    "task = 'Given a math problem statement and an incorrect answer as a query, retrieve relevant passages that identify and explain the nature of the error.'\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    # From https://github.com/UKPLab/sentence-transformers/blob/master/\n",
    "    # sentence_transformers/util.py#L31\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "\n",
    "class MultipleNegativesRankingLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, embeddings_a, embeddings_b, labels=None):\n",
    "        \"\"\"\n",
    "        Compute similarity between `a` and `b`.\n",
    "        Labels have the index of the row number at each row.\n",
    "        This indicates that `a_i` and `b_j` have high similarity\n",
    "        when `i==j` and low similarity when `i!=j`.\n",
    "        \"\"\"\n",
    "\n",
    "        similarity_scores = (\n",
    "            cos_sim(embeddings_a, embeddings_b) * 20.0\n",
    "        )  # Not too sure why to scale it by 20:\n",
    "        # https://github.com/UKPLab/sentence-transformers/\n",
    "        # blob/b86eec31cf0a102ad786ba1ff31bfeb4998d3ca5/sentence_transformers/\n",
    "        # losses/MultipleNegativesRankingLoss.py#L57\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            range(len(similarity_scores)),\n",
    "            dtype=torch.long,\n",
    "            device=similarity_scores.device,\n",
    "        )  # Example a[i] should match with b[i]\n",
    "\n",
    "        return self.loss_function(similarity_scores, labels)\n",
    "\n",
    "\n",
    "def collate_sentence(d):\n",
    "    mask_len = int(d[\"attention_mask\"].sum(axis=1).max())\n",
    "    return {\"input_ids\": d['input_ids'][:, :mask_len],\n",
    "            \"attention_mask\": d['attention_mask'][:, :mask_len],\n",
    "            \"token_type_ids\": d[\"token_type_ids\"][:, :mask_len]}\n",
    "\n",
    "\n",
    "def make_candidate_first_stage_for_val(val, misconception,\n",
    "                                       model, tokenizer, max_len,\n",
    "                                       batch_size, n_neighbor=100):\n",
    "    val_ = EediValDataset(val[\"all_text\"],\n",
    "                          tokenizer,\n",
    "                          max_len)\n",
    "    misconception_ = EediValDataset(misconception[\"MisconceptionName\"],\n",
    "                                    tokenizer,\n",
    "                                    max_len)\n",
    "    # make val emb\n",
    "    val_loader = DataLoader(\n",
    "        val_, batch_size=batch_size * 2, shuffle=False)\n",
    "    val_emb = make_emb(model, val_loader)\n",
    "\n",
    "    # make misconception emb\n",
    "    misconcept_loader = DataLoader(\n",
    "        misconception_, batch_size=batch_size * 2, shuffle=False)\n",
    "    misconcept_emb = make_emb(model, misconcept_loader)\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbor,\n",
    "                           metric=\"cosine\")\n",
    "    knn.fit(misconcept_emb)\n",
    "    dists, nears = knn.kneighbors(val_emb)\n",
    "    return nears\n",
    "\n",
    "\n",
    "def make_emb(model, train_loader):\n",
    "    bert_emb = []\n",
    "    with torch.no_grad():\n",
    "        for d in train_loader:\n",
    "            d = collate_sentence(d)\n",
    "            input_ids = d['input_ids']\n",
    "            mask = d['attention_mask']\n",
    "            input_ids = input_ids.to(device)\n",
    "            mask = mask.to(device)\n",
    "            output = model(input_ids, mask)\n",
    "            output = output.detach().cpu().numpy().astype(np.float32)\n",
    "            bert_emb.append(output)\n",
    "    torch.cuda.empty_cache()\n",
    "    bert_emb = np.concatenate(bert_emb)\n",
    "    return bert_emb\n",
    "\n",
    "\n",
    "def calculate_map25_with_metrics(df):\n",
    "    def ap_at_k(actual, predicted, k=25):\n",
    "        actual = int(actual)\n",
    "        predicted = predicted[:k]\n",
    "        score = 0.0\n",
    "        num_hits = 0.0\n",
    "        found = False\n",
    "        rank = None\n",
    "        for i, p in enumerate(predicted):\n",
    "            if p == actual:\n",
    "                if not found:\n",
    "                    found = True\n",
    "                    rank = i + 1\n",
    "                num_hits += 1\n",
    "                score += num_hits / (i + 1.0)\n",
    "        return score, found, rank\n",
    "\n",
    "    scores = []\n",
    "    found_count = 0\n",
    "    rankings = []\n",
    "    total_count = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        actual = row['MisconceptionId']\n",
    "        predicted = [int(float(x)) for x in row['pred'].split()]\n",
    "        score, found, rank = ap_at_k(actual, predicted)\n",
    "        scores.append(score)\n",
    "\n",
    "        total_count += 1\n",
    "        if found:\n",
    "            found_count += 1\n",
    "            rankings.append(rank)\n",
    "\n",
    "    map25 = np.mean(scores)\n",
    "    percent_found = (found_count / total_count) * 100 if total_count > 0 else 0\n",
    "    avg_ranking = np.mean(rankings) if rankings else 0\n",
    "\n",
    "    return map25, percent_found, avg_ranking\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger()\n",
    "FORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "def setup_logger(out_file=None, stderr=True,\n",
    "                 stderr_level=logging.INFO, file_level=logging.DEBUG):\n",
    "    LOGGER.handlers = []\n",
    "    LOGGER.setLevel(min(stderr_level, file_level))\n",
    "\n",
    "    if stderr:\n",
    "        handler = logging.StreamHandler(sys.stderr)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(stderr_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        handler = logging.FileHandler(out_file)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(file_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    LOGGER.info(\"logger set up\")\n",
    "    return LOGGER\n",
    "\n",
    "\n",
    "@ contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "\n",
    "setup_logger(out_file=logger_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64dca50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# main\n",
    "# ============================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "misconception = pd.read_csv(MISCONCEPTION_MAPPING_PATH)\n",
    "llm_text = pd.read_csv(LLM_TEXT_PATH)\n",
    "\n",
    "train_pivot = []\n",
    "common_cols = ['QuestionId', 'ConstructId', 'ConstructName', 'SubjectId',\n",
    "               'SubjectName', 'CorrectAnswer', 'QuestionText']\n",
    "for i in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    train_ = train.copy()\n",
    "    train_ = train[common_cols + [f\"Answer{i}Text\", f\"Misconception{i}Id\"]]\n",
    "    train_ = train_.rename({f\"Answer{i}Text\": \"AnswerText\",\n",
    "                            f\"Misconception{i}Id\": \"MisconceptionId\"}, axis=1)\n",
    "    train_[\"ans\"] = i\n",
    "    train_pivot.append(train_)\n",
    "\n",
    "train_pivot = pd.concat(train_pivot).reset_index(drop=True)\n",
    "train_pivot = train_pivot[train_pivot[\"MisconceptionId\"].notnull()].reset_index(\n",
    "    drop=True)\n",
    "\n",
    "train_pivot = train_pivot.merge(\n",
    "    llm_text[[\"QuestionId\", \"ans\", \"llmMisconception\"]], how=\"left\", on=[\"QuestionId\", \"ans\"])\n",
    "\n",
    "train_pivot[\"all_text\"] = ' <Question> ' + train_pivot['QuestionText'] + \\\n",
    "    ' <Answer> ' + train_pivot['AnswerText'] + \\\n",
    "    '<Construct> ' + train_pivot['ConstructName'] + \\\n",
    "                          ' <Subject> ' + train_pivot['SubjectName'] + \\\n",
    "                          ' <LLMOutput> ' + train_pivot['llmMisconception']\n",
    "train_pivot[\"MisconceptionId\"] = train_pivot[\"MisconceptionId\"].astype(int)\n",
    "train_pivot = train_pivot.merge(\n",
    "    misconception, how=\"left\", on=\"MisconceptionId\")\n",
    "\n",
    "text_list = []\n",
    "for t in train_pivot[\"all_text\"].values:\n",
    "    text_list.append(get_detailed_instruct(task, t))\n",
    "train_pivot[\"all_text\"] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9ad64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "    \n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference_2nd_stage(model, sentences, misconceptions,tokenizer,\n",
    "                        device, batch_size=16, max_length=384):\n",
    "    sentences_emb = []\n",
    "    for start_index in range(0, len(sentences), batch_size):\n",
    "        sentences_batch = sentences[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().astype(np.float32)\n",
    "        sentences_emb.append(embeddings)\n",
    "    sentences_emb = np.concatenate(sentences_emb,axis=0)\n",
    "    misconception_emb = []\n",
    "    for start_index in range(0, len(misconceptions), batch_size):\n",
    "        sentences_batch = misconceptions[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().astype(np.float32)\n",
    "        misconception_emb.append(embeddings)\n",
    "    \n",
    "    misconception_emb = np.concatenate(misconception_emb,axis=0)\n",
    "    return sentences_emb,misconception_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a21bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fold = pd.read_csv(FOLD_PATH)\n",
    "df_fold = df_fold.drop_duplicates(subset=[\"QuestionId\"]).reset_index(drop=True)\n",
    "train_pivot = train_pivot.merge(\n",
    "    df_fold[[\"QuestionId\", \"fold\"]], how=\"left\", on=\"QuestionId\")\n",
    "fold_array = train_pivot[\"fold\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67feeb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "fold0: val_score 0.46098765650068674 recall 0.8718535469107551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "fold1: val_score 0.45989242495224447 recall 0.8867276887871853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "fold2: val_score 0.45546207730556215 recall 0.8924485125858124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "fold3: val_score 0.4810161725387437 recall 0.9084668192219679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]\n",
      "2024-11-24 10:09:10,288 - INFO - [train] done in 265 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "fold4: val_score 0.4515055110857205 recall 0.8867276887871853\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# eval\n",
    "# ================================\n",
    "with timer(\"train\"):\n",
    "    set_seed(seed)\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    val_pred_all = []\n",
    "    recall_list = []\n",
    "    for n in range(5):\n",
    "        x_val = train_pivot[fold_array == n].reset_index(drop=True)\n",
    "        lora_path = model_dir / f\"fold{n}/adapter.bin\"\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"fp4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "        model = AutoModel.from_pretrained(model_path, quantization_config=bnb_config,device_map=device)\n",
    "        config = LoraConfig(\n",
    "            r=32,\n",
    "            lora_alpha=64,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            bias=\"none\",\n",
    "            lora_dropout=0.05,  # Conventional\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        d = torch.load(lora_path, map_location=model.device)\n",
    "        model.load_state_dict(d, strict=False)\n",
    "        model = model.eval()\n",
    "        batch_size = 12\n",
    "        max_length = 384\n",
    "        sentences = list(x_val['all_text'].values)\n",
    "        misconception_list = list(misconception['MisconceptionName'].values)\n",
    "        sentences_emb,misconception_emb = inference_2nd_stage(model, sentences, misconception_list,tokenizer,\n",
    "                        device)\n",
    "        sentence_emb1 = np.load(f\"/tmp/working/storage/eedi/output/exp/ex{exp1}/exp{exp1}_{exp2}_{n}_val_emb.npy\")\n",
    "        misconception_emb1 = np.load(f\"/tmp/working/storage/eedi/output/exp/ex{exp1}/exp{exp1}_{exp2}_{n}_misconcept_emb.npy\")\n",
    "        sentences_emb = np.concatenate([sentences_emb,sentence_emb1],axis=1)\n",
    "        misconception_emb = np.concatenate([misconception_emb,misconception_emb1],axis=1)\n",
    "        knn = NearestNeighbors(n_neighbors=50,\n",
    "                           metric=\"cosine\")\n",
    "        knn.fit(misconception_emb)\n",
    "        dists, pred = knn.kneighbors(sentences_emb)\n",
    "        recall = 0\n",
    "        for gt,p in zip(x_val[\"MisconceptionId\"],pred[:,:25]):\n",
    "            if gt in p:\n",
    "                recall += 1\n",
    "        recall /= len(x_val)\n",
    "        recall_list.append(recall)\n",
    "        pred_  = []\n",
    "        for i in pred:\n",
    "            pred_.append(' '.join(map(str, i)))\n",
    "\n",
    "        val_pred = pd.DataFrame()\n",
    "        val_pred[\"MisconceptionId\"] = x_val[\"MisconceptionId\"]\n",
    "        val_pred[\"pred\"] = pred_\n",
    "        val_pred[\"QuestionId\"] = x_val[\"QuestionId\"]\n",
    "        val_pred[\"ans\"] = x_val[\"ans\"]\n",
    "        val_pred[\"fold\"] = n\n",
    "        print(n)\n",
    "        val_score, percent_found, avg_ranking = calculate_map25_with_metrics(val_pred)\n",
    "        print(f\"fold{n}: val_score {val_score} recall {recall}\")\n",
    "\n",
    "        val_pred_all.append(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daf0c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_all = pd.concat(val_pred_all,axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65b988e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_all.to_parquet(exp_dir / f\"exp{exp}_val_pred_239_240_241.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de73608e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4617727684765915, 88.92448512585813, 4.525218733916624)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_map25_with_metrics(val_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a079c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
